import sys
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.utils import getResolvedOptions
from awsglue.job import Job
# TODO: make imports explicit.  Do not use glob include here.
from awsglue.transforms import * 

args = getResolvedOptions(sys.argv, ['JOB_NAME','SOURCE_DB_NAME','SOURCE_TABLE_NAME','SOURCE_S3_OBJECT_PATH','TARGET_BUCKET_NAME'])
source_db_name = args['SOURCE_DB_NAME']
source_table_name = args['SOURCE_TABLE_NAME']
source_s3_object_path = args['SOURCE_S3_OBJECT_PATH']
target_bucket_name = args['TARGET_BUCKET_NAME']

def replace_dots(string):
    '''replace dots with underscore in string'''
    return string.replace("val.","").replace(".", "_")


def get_dataframe_collection_from_athena_table(db_name, table_name, source_s3_object_path):
    '''
    Use relationalize function to flatten the data by creating individual data
    frame for each array associated with the main dynamic dataframe network_data.
    These dataframes are collectively grouped into one data frame collection
    (dfc).
    '''

    glueContext = GlueContext(SparkContext.getOrCreate())
    catalog_data = glueContext.create_dynamic_frame.from_catalog(
        database=db_name,
        table_name=table_name,
    )
    #catalog_data.printSchema()

    dfc = catalog_data.relationalize("data_root", "s3://" + source_s3_object_path + "/")
    #print(dfc.keys())
    return dfc


def load_dataframe_as_orc_file_to_s3(dfc, df_name, bucket_name):
    '''
    Write dynamic frame into s3 bucket with the same name as dataframe.
    The content of the file in the bucket is in the form of orc file.
    '''
    glueContext = GlueContext(SparkContext.getOrCreate())
    df = dfc.select(df_name)
    new_df = df.toDF()
    print (type( new_df))
    for oldName in new_df.schema.names:
        new_df = new_df.withColumnRenamed(oldName, oldName.replace("val.","").replace(".","_"))
    df = df.fromDF(new_df, glueContext, "df")
    response = glueContext.write_dynamic_frame.from_options(
        frame = df,
        connection_type = "s3",
        connection_options = {"path": "s3://" + bucket_name + "/" + replace_dots(df_name) + "/"},
        format = "orc",
        transformation_ctx = replace_dots(df_name),
    )


# TODO: add click paramater parsing here, with descriptions of parameters
def main(source_db_name, source_table_name, source_s3_object_path, target_bucket_name):
    dfc = get_dataframe_collection_from_athena_table(source_db_name, source_table_name, source_s3_object_path)
    for df_name in dfc.keys():
        load_dataframe_as_orc_file_to_s3(dfc, df_name, target_bucket_name)


if __name__ == "__main__":
    
    main(source_db_name, source_table_name, source_s3_object_path, target_bucket_name)